---
title: "Practical Machine Learning Course Project: quantifying exercise quality"
author: "Chunle Xiong"
date: "17 January 2017"
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

Using modern devices people can collect a large amount of data about personal exercise activities. Usually this can only quantify how much of a particular activity they do, but they rarely quantify how well they do it. The question that the machine learning prediction model in this course project will answer is how quantitatively well people do activities according to their activity pattern.

Since the question has been identified, the next step is to choose the most relevant data to build the prediction model. We will use data from accelerometers on the belt, forearm, arm, and dumbell of six male young health participants aged between 20-28 years. They were asked to perform one set of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

By choosing the most relevant measurement data and building a proper prediction model, we can predict which class a specified participant's activities belong to and thus quantify how well he does the activities.

## Loading the data sets and relevant packages
After having a quick exploratory analysis of the data sets by simply reading them using default read.csv settings, we have found a lot of missing values in the forms of "NA", "#DIV/0!" and "". So we load the data sets in the following way.

```{r loaddata, cache=TRUE}
#Loading the csv files that have been saved in the working directory
training <- read.csv(paste(getwd(), "/pml-training.csv", sep = ""), na.strings = c("NA", "#DIV/0!", ""))
quiz <- read.csv(paste(getwd(), "/pml-testing.csv", sep = ""), na.strings = c("NA", "#DIV/0!", ""))
#Loading the required packages for analysis
library(caret)
library(rpart)
library(rattle)
library(parallel)
library(doParallel)
library(randomForest)
```

## Cleaning the data sets
A further exploratory analysis shows that some columns include a large portion of missing values (i.e., NA) and the first column only contains the row number which is irrelevant to the analysis. The first step of cleaning data is to remove these columns. The method used here removes the columns that contain at least one NA. This method works well for this particular project because the removed columns contain many NAs. Other methods need to be considered if only very few NAs are in the data sets.

```{r cleandata1, cache=TRUE}
#Only getting the index, keeping the data sets unchanged
good_index <- colnames(training[colSums(is.na(training)) == 0])
good_index <- good_index[-1]
```

The second step of cleaning data is to remove NearZeroVariance variables using the following R code.

```{r cleandata2, cache=TRUE}
myDataNZV <- nearZeroVar(training[good_index], saveMetrics=TRUE)
#myDataNZV shows column 5 correspons to NearZeroVariance variables
good_index <- good_index[-5]
```

## Splitting the data set to training and testing sets
To make the analysis reproducible, we set a seed for data splitting and only keep the good data in the training and testing sets for analysis and prediction.

```{r splitdata, cache=TRUE}
set.seed(1357)
inTrain <- createDataPartition(training$classe, p = 0.6, list = FALSE)
#Only keeping the good data
mytraining <- training[inTrain, good_index]
mytesting <- training[-inTrain, good_index]
dim(training)
dim(mytraining)
dim(mytesting)
```

It can be seen from the displayed dimensions that the data sets that will be used for analysis have reduced number of variables from 160 to 58. The data cleaning does not only remove the irrelevant data and also makes the following analysis much more efficient.

## Decision tree analysis

## Use parallel implementation of Random Forest

Next we try Random Forest to see if the accuracy can be improved. To make the modelling quicker, we choose the parallel implementation of Random Forest in the caret package with the resampling method changed from the default bootstrapping to 4-fold cross-validation.

```{r randomforest, cache=TRUE}
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)

fitControl <- trainControl(method = "cv", number = 4, allowParallel = TRUE)
set.seed(2468)
fit <- train(classe ~ ., method="rf", data = mytraining, trControl = fitControl)
stopCluster(cluster)
registerDoSEQ()
```

## Out of sample error

Now we check the accuracy of the model. It can be seen that the accuracy is as high as 99.92% even applying the model to the testing sample set that is out of the training data set. This indicates that the out of sample error is as low as 0.08%. It should be noted that even though the testing data set is out of training data set, they belong to the same original data set. This means these data will have high degrees of consistency. If we apply the model to some data that is completely different from the original data set, for example the participants are different people, we might get less accurate prediction.

```{r outerror, cache=TRUE}
prediction <- predict(fit, mytesting)
confusionMatrix(prediction, mytesting$classe)
```

## Apply the model to the quiz set

```{r quiz, cache=TRUE}
myquiz <- quiz
colnames(myquiz)[160] <- "classe"
myquiz <- myquiz[, good_index]
myquiz_pred <- predict(fit, myquiz)
print(myquiz_pred)
```

## Conclusion
The Random Forest model provides much more accurate prediction for the exercise quality data than the rpart model, and the accuracy is as high as 99.92%. It should be noted that for completely different data sets, the model might not be able to provide as accurate prediction as for the one from the same population that the training set is from.